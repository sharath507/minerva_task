{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM37e6/dq6p9GsMsCnvzZTn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharath507/minerva_task/blob/main/minerva_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "BIBCm_c15OY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generating synthetic data for demonstration\n",
        "# Replace with your dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.randint(40, 100, size=(1000, 6))  # Past 6 years' scores\n",
        "y = np.random.randint(40, 100, size=(1000, 1))"
      ],
      "metadata": {
        "id": "Qaf1wtkt5QV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( X[:10])\n",
        "print(y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq2LDKE95vV0",
        "outputId": "8c55ea0d-a33e-4f37-e12d-347a6a32c063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[78 91 68 54 82 47]\n",
            " [60 78 97 58 62 50]\n",
            " [50 63 92 75 79 63]\n",
            " [42 61 92 41 63 83]\n",
            " [69 77 41 99 60 72]\n",
            " [51 97 61 83 64 88]\n",
            " [66 98 81 67 99 55]\n",
            " [54 86 90 83 94 91]\n",
            " [96 42 76 90 46 60]\n",
            " [48 78 57 43 64 99]]\n",
            "[[88]\n",
            " [60]\n",
            " [48]\n",
            " [45]\n",
            " [90]\n",
            " [84]\n",
            " [74]\n",
            " [78]\n",
            " [61]\n",
            " [80]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "8A4C9c2W5W7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[:10])\n",
        "print(y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Nd-15v6K3D",
        "outputId": "ae7962b5-917c-42eb-fed0-e5833446e0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[52 80 62 58 84 50]\n",
            " [69 45 68 47 83 84]\n",
            " [92 74 80 98 53 69]\n",
            " [77 88 40 64 90 75]\n",
            " [54 90 52 58 88 45]\n",
            " [62 83 78 54 95 88]\n",
            " [41 74 57 53 59 57]\n",
            " [95 88 52 77 88 85]\n",
            " [83 41 63 73 66 94]\n",
            " [64 78 88 99 67 72]]\n",
            "[[71]\n",
            " [71]\n",
            " [60]\n",
            " [67]\n",
            " [67]\n",
            " [78]\n",
            " [93]\n",
            " [64]\n",
            " [57]\n",
            " [94]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "VP2IW6AU5bU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[:10])\n",
        "print(y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOEKBDtv6gL3",
        "outputId": "55774a5b-db8d-40a3-fded-0ae7da75ce72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[52 80 62 58 84 50]\n",
            " [69 45 68 47 83 84]\n",
            " [92 74 80 98 53 69]\n",
            " [77 88 40 64 90 75]\n",
            " [54 90 52 58 88 45]\n",
            " [62 83 78 54 95 88]\n",
            " [41 74 57 53 59 57]\n",
            " [95 88 52 77 88 85]\n",
            " [83 41 63 73 66 94]\n",
            " [64 78 88 99 67 72]]\n",
            "[[71]\n",
            " [71]\n",
            " [60]\n",
            " [67]\n",
            " [67]\n",
            " [78]\n",
            " [93]\n",
            " [64]\n",
            " [57]\n",
            " [94]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(64, input_dim=6, activation='relu'),  # Hidden layer 1\n",
        "    Dense(32, activation='relu'),               # Hidden layer 2\n",
        "    Dense(1)                                    # Output layer (single value for marks)\n",
        "])\n"
      ],
      "metadata": {
        "id": "byEwba6G5ejK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FSSXnRVd5lpo",
        "outputId": "ca1573ee-b23c-43c5-feb4-ca4e3c4869ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 600.3050 - mae: 20.3559 - val_loss: 421.8697 - val_mae: 17.4017\n",
            "Epoch 2/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 373.8133 - mae: 16.0124 - val_loss: 376.3673 - val_mae: 16.6684\n",
            "Epoch 3/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 334.9514 - mae: 15.0875 - val_loss: 371.3827 - val_mae: 16.5717\n",
            "Epoch 4/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 344.2609 - mae: 15.6109 - val_loss: 380.6636 - val_mae: 16.7792\n",
            "Epoch 5/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 335.6980 - mae: 15.4353 - val_loss: 388.5323 - val_mae: 16.9027\n",
            "Epoch 6/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 340.7214 - mae: 15.4516 - val_loss: 369.5733 - val_mae: 16.5314\n",
            "Epoch 7/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 350.0898 - mae: 15.5227 - val_loss: 378.5421 - val_mae: 16.8016\n",
            "Epoch 8/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 347.2990 - mae: 15.6970 - val_loss: 394.6076 - val_mae: 17.0386\n",
            "Epoch 9/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 322.0351 - mae: 14.9276 - val_loss: 389.4126 - val_mae: 16.9608\n",
            "Epoch 10/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 326.5309 - mae: 15.0992 - val_loss: 374.6151 - val_mae: 16.6521\n",
            "Epoch 11/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 346.8238 - mae: 15.7287 - val_loss: 378.9023 - val_mae: 16.7502\n",
            "Epoch 12/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 349.6557 - mae: 15.7890 - val_loss: 392.3949 - val_mae: 17.0066\n",
            "Epoch 13/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.7272 - mae: 15.0355 - val_loss: 382.7000 - val_mae: 16.8346\n",
            "Epoch 14/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 339.2716 - mae: 15.4086 - val_loss: 374.3539 - val_mae: 16.6192\n",
            "Epoch 15/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 317.0193 - mae: 15.0075 - val_loss: 375.6197 - val_mae: 16.6820\n",
            "Epoch 16/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 339.4333 - mae: 15.5584 - val_loss: 389.5562 - val_mae: 16.9275\n",
            "Epoch 17/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.4060 - mae: 15.1211 - val_loss: 387.2737 - val_mae: 16.9223\n",
            "Epoch 18/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 333.3019 - mae: 15.3203 - val_loss: 383.3775 - val_mae: 16.8318\n",
            "Epoch 19/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 307.0691 - mae: 14.7346 - val_loss: 386.8952 - val_mae: 16.9035\n",
            "Epoch 20/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 328.0807 - mae: 15.2098 - val_loss: 377.4248 - val_mae: 16.6913\n",
            "Epoch 21/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 341.0562 - mae: 15.6039 - val_loss: 374.6115 - val_mae: 16.6442\n",
            "Epoch 22/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 341.2838 - mae: 15.6567 - val_loss: 399.2081 - val_mae: 17.0989\n",
            "Epoch 23/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.0164 - mae: 15.2593 - val_loss: 377.1642 - val_mae: 16.7057\n",
            "Epoch 24/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 335.5306 - mae: 15.5629 - val_loss: 384.7614 - val_mae: 16.8072\n",
            "Epoch 25/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 344.8240 - mae: 15.5260 - val_loss: 393.3679 - val_mae: 16.9981\n",
            "Epoch 26/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 319.1617 - mae: 14.9667 - val_loss: 373.0350 - val_mae: 16.5970\n",
            "Epoch 27/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 319.8716 - mae: 15.0146 - val_loss: 379.8272 - val_mae: 16.6679\n",
            "Epoch 28/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 329.4549 - mae: 15.1869 - val_loss: 376.2071 - val_mae: 16.6245\n",
            "Epoch 29/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 349.2310 - mae: 15.6631 - val_loss: 384.8641 - val_mae: 16.8348\n",
            "Epoch 30/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 332.4301 - mae: 15.3007 - val_loss: 376.2847 - val_mae: 16.6641\n",
            "Epoch 31/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 336.9912 - mae: 15.5331 - val_loss: 375.9248 - val_mae: 16.6075\n",
            "Epoch 32/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 329.8001 - mae: 15.2111 - val_loss: 380.0945 - val_mae: 16.7209\n",
            "Epoch 33/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 331.8781 - mae: 15.2646 - val_loss: 382.8604 - val_mae: 16.7972\n",
            "Epoch 34/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 302.6291 - mae: 14.5421 - val_loss: 376.0830 - val_mae: 16.6644\n",
            "Epoch 35/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 332.6942 - mae: 15.3494 - val_loss: 374.1508 - val_mae: 16.6164\n",
            "Epoch 36/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.0969 - mae: 15.1174 - val_loss: 377.9274 - val_mae: 16.7115\n",
            "Epoch 37/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 300.0606 - mae: 14.5090 - val_loss: 402.0031 - val_mae: 17.1118\n",
            "Epoch 38/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 322.4883 - mae: 14.9038 - val_loss: 387.8736 - val_mae: 16.9045\n",
            "Epoch 39/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 314.9359 - mae: 14.9296 - val_loss: 384.3873 - val_mae: 16.8088\n",
            "Epoch 40/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.7246 - mae: 15.1024 - val_loss: 377.1816 - val_mae: 16.7135\n",
            "Epoch 41/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 327.7078 - mae: 15.3413 - val_loss: 382.3082 - val_mae: 16.7511\n",
            "Epoch 42/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 327.8892 - mae: 15.2341 - val_loss: 410.2547 - val_mae: 17.2433\n",
            "Epoch 43/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 330.5614 - mae: 15.0867 - val_loss: 398.2109 - val_mae: 17.0490\n",
            "Epoch 44/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 327.7513 - mae: 15.2037 - val_loss: 389.7860 - val_mae: 16.9345\n",
            "Epoch 45/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 316.2640 - mae: 14.8122 - val_loss: 376.3084 - val_mae: 16.6482\n",
            "Epoch 46/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 316.5876 - mae: 14.9497 - val_loss: 374.8097 - val_mae: 16.6257\n",
            "Epoch 47/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.0090 - mae: 15.0273 - val_loss: 379.0103 - val_mae: 16.6996\n",
            "Epoch 48/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 328.1645 - mae: 15.0389 - val_loss: 387.6049 - val_mae: 16.8922\n",
            "Epoch 49/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 334.9643 - mae: 15.4885 - val_loss: 403.1875 - val_mae: 17.1347\n",
            "Epoch 50/50\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 324.5654 - mae: 15.0249 - val_loss: 378.6741 - val_mae: 16.7167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
        "\n",
        "# Predicting future scores\n",
        "y_pred = model.predict(X_test[:5])\n",
        "print(\"Predicted scores:\", y_pred.flatten())\n",
        "print(\"Actual scores:\", y_test[:5].flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5ib-1W05ocK",
        "outputId": "6c9c21cd-6896-48d1-8262-575bf527625f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 378.6741, Test MAE: 16.7167\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Predicted scores: [63.615536 62.846718 75.289246 76.15615  66.32548 ]\n",
            "Actual scores: [71 71 60 67 67]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give the code to display the variable x&y\n",
        "\n",
        "print(\"X:\", X)\n",
        "print(\"y:\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JFYb-UolU8Fn",
        "outputId": "ea35f98d-80ca-44c2-a3c5-7edbae0fab0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: [[78 91 68 54 82 47]\n",
            " [60 78 97 58 62 50]\n",
            " [50 63 92 75 79 63]\n",
            " ...\n",
            " [63 64 84 94 85 71]\n",
            " [94 71 44 75 85 81]\n",
            " [88 81 97 78 55 72]]\n",
            "y: [[88]\n",
            " [60]\n",
            " [48]\n",
            " [45]\n",
            " [90]\n",
            " [84]\n",
            " [74]\n",
            " [78]\n",
            " [61]\n",
            " [80]\n",
            " [93]\n",
            " [44]\n",
            " [81]\n",
            " [62]\n",
            " [56]\n",
            " [51]\n",
            " [62]\n",
            " [55]\n",
            " [42]\n",
            " [69]\n",
            " [96]\n",
            " [61]\n",
            " [48]\n",
            " [68]\n",
            " [86]\n",
            " [76]\n",
            " [62]\n",
            " [52]\n",
            " [71]\n",
            " [85]\n",
            " [91]\n",
            " [86]\n",
            " [87]\n",
            " [62]\n",
            " [84]\n",
            " [56]\n",
            " [94]\n",
            " [79]\n",
            " [60]\n",
            " [41]\n",
            " [76]\n",
            " [45]\n",
            " [83]\n",
            " [57]\n",
            " [69]\n",
            " [61]\n",
            " [93]\n",
            " [72]\n",
            " [69]\n",
            " [56]\n",
            " [82]\n",
            " [71]\n",
            " [81]\n",
            " [90]\n",
            " [53]\n",
            " [97]\n",
            " [49]\n",
            " [86]\n",
            " [99]\n",
            " [90]\n",
            " [75]\n",
            " [73]\n",
            " [48]\n",
            " [81]\n",
            " [67]\n",
            " [50]\n",
            " [53]\n",
            " [78]\n",
            " [94]\n",
            " [86]\n",
            " [93]\n",
            " [58]\n",
            " [93]\n",
            " [54]\n",
            " [67]\n",
            " [63]\n",
            " [47]\n",
            " [82]\n",
            " [48]\n",
            " [86]\n",
            " [72]\n",
            " [72]\n",
            " [58]\n",
            " [42]\n",
            " [49]\n",
            " [79]\n",
            " [86]\n",
            " [81]\n",
            " [40]\n",
            " [68]\n",
            " [89]\n",
            " [73]\n",
            " [90]\n",
            " [52]\n",
            " [93]\n",
            " [40]\n",
            " [73]\n",
            " [68]\n",
            " [87]\n",
            " [86]\n",
            " [52]\n",
            " [87]\n",
            " [88]\n",
            " [92]\n",
            " [68]\n",
            " [78]\n",
            " [63]\n",
            " [65]\n",
            " [56]\n",
            " [46]\n",
            " [52]\n",
            " [44]\n",
            " [65]\n",
            " [88]\n",
            " [42]\n",
            " [46]\n",
            " [97]\n",
            " [88]\n",
            " [78]\n",
            " [92]\n",
            " [40]\n",
            " [88]\n",
            " [56]\n",
            " [82]\n",
            " [64]\n",
            " [98]\n",
            " [42]\n",
            " [54]\n",
            " [68]\n",
            " [92]\n",
            " [82]\n",
            " [71]\n",
            " [65]\n",
            " [99]\n",
            " [55]\n",
            " [64]\n",
            " [93]\n",
            " [67]\n",
            " [48]\n",
            " [95]\n",
            " [95]\n",
            " [40]\n",
            " [68]\n",
            " [96]\n",
            " [61]\n",
            " [55]\n",
            " [80]\n",
            " [90]\n",
            " [83]\n",
            " [90]\n",
            " [93]\n",
            " [80]\n",
            " [46]\n",
            " [53]\n",
            " [68]\n",
            " [58]\n",
            " [58]\n",
            " [41]\n",
            " [80]\n",
            " [84]\n",
            " [54]\n",
            " [42]\n",
            " [50]\n",
            " [92]\n",
            " [85]\n",
            " [69]\n",
            " [96]\n",
            " [79]\n",
            " [57]\n",
            " [90]\n",
            " [87]\n",
            " [41]\n",
            " [71]\n",
            " [47]\n",
            " [61]\n",
            " [45]\n",
            " [51]\n",
            " [86]\n",
            " [96]\n",
            " [79]\n",
            " [75]\n",
            " [54]\n",
            " [67]\n",
            " [91]\n",
            " [96]\n",
            " [88]\n",
            " [84]\n",
            " [57]\n",
            " [41]\n",
            " [52]\n",
            " [84]\n",
            " [93]\n",
            " [54]\n",
            " [59]\n",
            " [68]\n",
            " [98]\n",
            " [81]\n",
            " [68]\n",
            " [42]\n",
            " [58]\n",
            " [42]\n",
            " [93]\n",
            " [53]\n",
            " [79]\n",
            " [98]\n",
            " [94]\n",
            " [43]\n",
            " [98]\n",
            " [99]\n",
            " [91]\n",
            " [50]\n",
            " [97]\n",
            " [70]\n",
            " [82]\n",
            " [71]\n",
            " [66]\n",
            " [69]\n",
            " [82]\n",
            " [41]\n",
            " [88]\n",
            " [48]\n",
            " [67]\n",
            " [70]\n",
            " [99]\n",
            " [73]\n",
            " [95]\n",
            " [54]\n",
            " [87]\n",
            " [64]\n",
            " [75]\n",
            " [47]\n",
            " [48]\n",
            " [97]\n",
            " [97]\n",
            " [76]\n",
            " [87]\n",
            " [84]\n",
            " [78]\n",
            " [71]\n",
            " [97]\n",
            " [91]\n",
            " [56]\n",
            " [74]\n",
            " [76]\n",
            " [58]\n",
            " [83]\n",
            " [68]\n",
            " [98]\n",
            " [46]\n",
            " [67]\n",
            " [48]\n",
            " [67]\n",
            " [45]\n",
            " [68]\n",
            " [97]\n",
            " [56]\n",
            " [55]\n",
            " [62]\n",
            " [70]\n",
            " [42]\n",
            " [60]\n",
            " [65]\n",
            " [75]\n",
            " [81]\n",
            " [76]\n",
            " [92]\n",
            " [61]\n",
            " [81]\n",
            " [51]\n",
            " [81]\n",
            " [52]\n",
            " [50]\n",
            " [93]\n",
            " [55]\n",
            " [47]\n",
            " [79]\n",
            " [98]\n",
            " [96]\n",
            " [79]\n",
            " [84]\n",
            " [98]\n",
            " [41]\n",
            " [69]\n",
            " [69]\n",
            " [51]\n",
            " [49]\n",
            " [83]\n",
            " [50]\n",
            " [54]\n",
            " [55]\n",
            " [56]\n",
            " [65]\n",
            " [41]\n",
            " [89]\n",
            " [51]\n",
            " [90]\n",
            " [65]\n",
            " [62]\n",
            " [57]\n",
            " [71]\n",
            " [88]\n",
            " [77]\n",
            " [96]\n",
            " [86]\n",
            " [42]\n",
            " [77]\n",
            " [42]\n",
            " [69]\n",
            " [40]\n",
            " [69]\n",
            " [84]\n",
            " [71]\n",
            " [67]\n",
            " [60]\n",
            " [97]\n",
            " [85]\n",
            " [52]\n",
            " [66]\n",
            " [75]\n",
            " [46]\n",
            " [61]\n",
            " [49]\n",
            " [78]\n",
            " [92]\n",
            " [54]\n",
            " [54]\n",
            " [43]\n",
            " [71]\n",
            " [62]\n",
            " [94]\n",
            " [45]\n",
            " [63]\n",
            " [59]\n",
            " [97]\n",
            " [76]\n",
            " [59]\n",
            " [51]\n",
            " [72]\n",
            " [67]\n",
            " [47]\n",
            " [40]\n",
            " [76]\n",
            " [61]\n",
            " [45]\n",
            " [60]\n",
            " [71]\n",
            " [74]\n",
            " [54]\n",
            " [44]\n",
            " [99]\n",
            " [95]\n",
            " [85]\n",
            " [89]\n",
            " [87]\n",
            " [45]\n",
            " [64]\n",
            " [95]\n",
            " [85]\n",
            " [98]\n",
            " [95]\n",
            " [79]\n",
            " [68]\n",
            " [46]\n",
            " [79]\n",
            " [52]\n",
            " [77]\n",
            " [57]\n",
            " [41]\n",
            " [47]\n",
            " [50]\n",
            " [72]\n",
            " [93]\n",
            " [77]\n",
            " [87]\n",
            " [66]\n",
            " [59]\n",
            " [51]\n",
            " [82]\n",
            " [70]\n",
            " [66]\n",
            " [71]\n",
            " [49]\n",
            " [42]\n",
            " [84]\n",
            " [66]\n",
            " [74]\n",
            " [70]\n",
            " [54]\n",
            " [55]\n",
            " [52]\n",
            " [88]\n",
            " [74]\n",
            " [91]\n",
            " [50]\n",
            " [98]\n",
            " [60]\n",
            " [90]\n",
            " [93]\n",
            " [50]\n",
            " [69]\n",
            " [99]\n",
            " [87]\n",
            " [92]\n",
            " [74]\n",
            " [61]\n",
            " [77]\n",
            " [70]\n",
            " [53]\n",
            " [63]\n",
            " [89]\n",
            " [88]\n",
            " [71]\n",
            " [75]\n",
            " [71]\n",
            " [94]\n",
            " [54]\n",
            " [82]\n",
            " [97]\n",
            " [74]\n",
            " [42]\n",
            " [55]\n",
            " [44]\n",
            " [60]\n",
            " [49]\n",
            " [58]\n",
            " [64]\n",
            " [43]\n",
            " [68]\n",
            " [58]\n",
            " [82]\n",
            " [41]\n",
            " [49]\n",
            " [82]\n",
            " [64]\n",
            " [59]\n",
            " [87]\n",
            " [58]\n",
            " [62]\n",
            " [63]\n",
            " [50]\n",
            " [85]\n",
            " [73]\n",
            " [94]\n",
            " [67]\n",
            " [44]\n",
            " [98]\n",
            " [44]\n",
            " [90]\n",
            " [60]\n",
            " [51]\n",
            " [89]\n",
            " [43]\n",
            " [93]\n",
            " [88]\n",
            " [81]\n",
            " [59]\n",
            " [77]\n",
            " [64]\n",
            " [53]\n",
            " [50]\n",
            " [71]\n",
            " [50]\n",
            " [57]\n",
            " [66]\n",
            " [94]\n",
            " [93]\n",
            " [57]\n",
            " [95]\n",
            " [62]\n",
            " [50]\n",
            " [60]\n",
            " [83]\n",
            " [44]\n",
            " [62]\n",
            " [80]\n",
            " [98]\n",
            " [64]\n",
            " [80]\n",
            " [93]\n",
            " [49]\n",
            " [61]\n",
            " [58]\n",
            " [40]\n",
            " [46]\n",
            " [43]\n",
            " [40]\n",
            " [54]\n",
            " [79]\n",
            " [51]\n",
            " [61]\n",
            " [61]\n",
            " [89]\n",
            " [86]\n",
            " [56]\n",
            " [68]\n",
            " [85]\n",
            " [53]\n",
            " [89]\n",
            " [59]\n",
            " [89]\n",
            " [87]\n",
            " [72]\n",
            " [94]\n",
            " [98]\n",
            " [82]\n",
            " [54]\n",
            " [51]\n",
            " [79]\n",
            " [93]\n",
            " [59]\n",
            " [69]\n",
            " [77]\n",
            " [42]\n",
            " [67]\n",
            " [95]\n",
            " [65]\n",
            " [69]\n",
            " [52]\n",
            " [52]\n",
            " [41]\n",
            " [76]\n",
            " [88]\n",
            " [77]\n",
            " [41]\n",
            " [75]\n",
            " [93]\n",
            " [65]\n",
            " [50]\n",
            " [47]\n",
            " [83]\n",
            " [79]\n",
            " [82]\n",
            " [61]\n",
            " [59]\n",
            " [47]\n",
            " [68]\n",
            " [79]\n",
            " [65]\n",
            " [69]\n",
            " [56]\n",
            " [59]\n",
            " [91]\n",
            " [94]\n",
            " [55]\n",
            " [85]\n",
            " [82]\n",
            " [46]\n",
            " [55]\n",
            " [84]\n",
            " [98]\n",
            " [86]\n",
            " [95]\n",
            " [87]\n",
            " [66]\n",
            " [92]\n",
            " [62]\n",
            " [63]\n",
            " [48]\n",
            " [74]\n",
            " [90]\n",
            " [59]\n",
            " [76]\n",
            " [81]\n",
            " [56]\n",
            " [86]\n",
            " [70]\n",
            " [94]\n",
            " [96]\n",
            " [47]\n",
            " [54]\n",
            " [70]\n",
            " [92]\n",
            " [69]\n",
            " [85]\n",
            " [59]\n",
            " [99]\n",
            " [78]\n",
            " [92]\n",
            " [94]\n",
            " [51]\n",
            " [88]\n",
            " [56]\n",
            " [81]\n",
            " [69]\n",
            " [68]\n",
            " [58]\n",
            " [85]\n",
            " [50]\n",
            " [73]\n",
            " [89]\n",
            " [46]\n",
            " [83]\n",
            " [97]\n",
            " [73]\n",
            " [98]\n",
            " [59]\n",
            " [89]\n",
            " [77]\n",
            " [63]\n",
            " [49]\n",
            " [71]\n",
            " [91]\n",
            " [54]\n",
            " [76]\n",
            " [59]\n",
            " [42]\n",
            " [76]\n",
            " [96]\n",
            " [79]\n",
            " [47]\n",
            " [79]\n",
            " [83]\n",
            " [95]\n",
            " [62]\n",
            " [79]\n",
            " [50]\n",
            " [85]\n",
            " [47]\n",
            " [68]\n",
            " [69]\n",
            " [95]\n",
            " [99]\n",
            " [49]\n",
            " [78]\n",
            " [67]\n",
            " [88]\n",
            " [75]\n",
            " [52]\n",
            " [71]\n",
            " [75]\n",
            " [76]\n",
            " [65]\n",
            " [61]\n",
            " [43]\n",
            " [70]\n",
            " [57]\n",
            " [77]\n",
            " [63]\n",
            " [95]\n",
            " [62]\n",
            " [61]\n",
            " [41]\n",
            " [70]\n",
            " [94]\n",
            " [50]\n",
            " [67]\n",
            " [85]\n",
            " [71]\n",
            " [44]\n",
            " [77]\n",
            " [49]\n",
            " [79]\n",
            " [85]\n",
            " [63]\n",
            " [74]\n",
            " [83]\n",
            " [87]\n",
            " [99]\n",
            " [62]\n",
            " [56]\n",
            " [40]\n",
            " [94]\n",
            " [70]\n",
            " [70]\n",
            " [81]\n",
            " [96]\n",
            " [94]\n",
            " [81]\n",
            " [92]\n",
            " [95]\n",
            " [73]\n",
            " [48]\n",
            " [61]\n",
            " [52]\n",
            " [64]\n",
            " [87]\n",
            " [57]\n",
            " [90]\n",
            " [51]\n",
            " [79]\n",
            " [56]\n",
            " [54]\n",
            " [81]\n",
            " [54]\n",
            " [54]\n",
            " [94]\n",
            " [78]\n",
            " [52]\n",
            " [56]\n",
            " [71]\n",
            " [48]\n",
            " [79]\n",
            " [59]\n",
            " [65]\n",
            " [66]\n",
            " [60]\n",
            " [57]\n",
            " [52]\n",
            " [55]\n",
            " [48]\n",
            " [51]\n",
            " [83]\n",
            " [77]\n",
            " [58]\n",
            " [99]\n",
            " [46]\n",
            " [96]\n",
            " [74]\n",
            " [87]\n",
            " [57]\n",
            " [55]\n",
            " [94]\n",
            " [78]\n",
            " [43]\n",
            " [98]\n",
            " [64]\n",
            " [71]\n",
            " [79]\n",
            " [52]\n",
            " [84]\n",
            " [56]\n",
            " [88]\n",
            " [76]\n",
            " [45]\n",
            " [58]\n",
            " [52]\n",
            " [96]\n",
            " [99]\n",
            " [65]\n",
            " [53]\n",
            " [70]\n",
            " [93]\n",
            " [43]\n",
            " [99]\n",
            " [78]\n",
            " [47]\n",
            " [64]\n",
            " [52]\n",
            " [71]\n",
            " [98]\n",
            " [77]\n",
            " [77]\n",
            " [95]\n",
            " [67]\n",
            " [92]\n",
            " [69]\n",
            " [42]\n",
            " [67]\n",
            " [42]\n",
            " [84]\n",
            " [93]\n",
            " [73]\n",
            " [55]\n",
            " [61]\n",
            " [71]\n",
            " [77]\n",
            " [47]\n",
            " [94]\n",
            " [47]\n",
            " [96]\n",
            " [45]\n",
            " [53]\n",
            " [71]\n",
            " [74]\n",
            " [49]\n",
            " [42]\n",
            " [98]\n",
            " [51]\n",
            " [88]\n",
            " [98]\n",
            " [70]\n",
            " [68]\n",
            " [44]\n",
            " [91]\n",
            " [64]\n",
            " [80]\n",
            " [70]\n",
            " [60]\n",
            " [97]\n",
            " [59]\n",
            " [78]\n",
            " [61]\n",
            " [43]\n",
            " [52]\n",
            " [84]\n",
            " [81]\n",
            " [89]\n",
            " [88]\n",
            " [59]\n",
            " [91]\n",
            " [52]\n",
            " [56]\n",
            " [93]\n",
            " [75]\n",
            " [72]\n",
            " [40]\n",
            " [54]\n",
            " [52]\n",
            " [56]\n",
            " [63]\n",
            " [97]\n",
            " [47]\n",
            " [89]\n",
            " [66]\n",
            " [84]\n",
            " [62]\n",
            " [85]\n",
            " [47]\n",
            " [47]\n",
            " [98]\n",
            " [65]\n",
            " [86]\n",
            " [98]\n",
            " [52]\n",
            " [78]\n",
            " [76]\n",
            " [88]\n",
            " [67]\n",
            " [64]\n",
            " [55]\n",
            " [94]\n",
            " [41]\n",
            " [93]\n",
            " [57]\n",
            " [56]\n",
            " [69]\n",
            " [78]\n",
            " [52]\n",
            " [71]\n",
            " [89]\n",
            " [68]\n",
            " [85]\n",
            " [69]\n",
            " [57]\n",
            " [52]\n",
            " [82]\n",
            " [60]\n",
            " [69]\n",
            " [60]\n",
            " [87]\n",
            " [94]\n",
            " [56]\n",
            " [79]\n",
            " [58]\n",
            " [91]\n",
            " [54]\n",
            " [48]\n",
            " [91]\n",
            " [49]\n",
            " [83]\n",
            " [70]\n",
            " [92]\n",
            " [62]\n",
            " [91]\n",
            " [69]\n",
            " [72]\n",
            " [96]\n",
            " [91]\n",
            " [48]\n",
            " [93]\n",
            " [86]\n",
            " [40]\n",
            " [83]\n",
            " [74]\n",
            " [50]\n",
            " [73]\n",
            " [55]\n",
            " [48]\n",
            " [71]\n",
            " [54]\n",
            " [51]\n",
            " [72]\n",
            " [84]\n",
            " [53]\n",
            " [96]\n",
            " [96]\n",
            " [47]\n",
            " [47]\n",
            " [66]\n",
            " [51]\n",
            " [40]\n",
            " [82]\n",
            " [79]\n",
            " [81]\n",
            " [90]\n",
            " [50]\n",
            " [78]\n",
            " [82]\n",
            " [70]\n",
            " [56]\n",
            " [72]\n",
            " [71]\n",
            " [97]\n",
            " [68]\n",
            " [60]\n",
            " [60]\n",
            " [79]\n",
            " [62]\n",
            " [95]\n",
            " [45]\n",
            " [61]\n",
            " [89]\n",
            " [77]\n",
            " [76]\n",
            " [84]\n",
            " [90]\n",
            " [93]\n",
            " [47]\n",
            " [80]\n",
            " [88]\n",
            " [74]\n",
            " [90]\n",
            " [50]\n",
            " [63]\n",
            " [88]\n",
            " [60]\n",
            " [90]\n",
            " [41]\n",
            " [79]\n",
            " [58]\n",
            " [78]\n",
            " [67]\n",
            " [98]\n",
            " [56]\n",
            " [45]\n",
            " [58]\n",
            " [95]\n",
            " [77]\n",
            " [67]\n",
            " [41]\n",
            " [77]\n",
            " [94]\n",
            " [76]\n",
            " [90]\n",
            " [91]\n",
            " [43]\n",
            " [61]\n",
            " [65]\n",
            " [88]\n",
            " [82]\n",
            " [75]\n",
            " [97]\n",
            " [52]\n",
            " [91]\n",
            " [97]\n",
            " [71]\n",
            " [56]\n",
            " [80]\n",
            " [75]\n",
            " [94]\n",
            " [99]\n",
            " [85]\n",
            " [58]\n",
            " [97]\n",
            " [99]\n",
            " [74]\n",
            " [85]\n",
            " [79]\n",
            " [95]\n",
            " [98]\n",
            " [67]\n",
            " [40]\n",
            " [81]\n",
            " [57]\n",
            " [87]\n",
            " [79]\n",
            " [72]\n",
            " [51]\n",
            " [47]\n",
            " [63]\n",
            " [55]\n",
            " [89]\n",
            " [43]\n",
            " [77]\n",
            " [83]\n",
            " [66]\n",
            " [48]\n",
            " [86]\n",
            " [55]\n",
            " [95]\n",
            " [51]\n",
            " [70]\n",
            " [63]\n",
            " [94]\n",
            " [53]\n",
            " [84]\n",
            " [64]\n",
            " [73]\n",
            " [42]\n",
            " [97]\n",
            " [63]\n",
            " [93]\n",
            " [81]\n",
            " [48]\n",
            " [65]\n",
            " [59]\n",
            " [79]\n",
            " [49]\n",
            " [58]\n",
            " [42]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give the code to concate the x and y varible into the table format\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Concatenate X and y into a DataFrame\n",
        "combined_data = np.concatenate((X, y), axis=1)\n",
        "df = pd.DataFrame(combined_data)\n",
        "\n",
        "# Add column names (adjust as needed)\n",
        "column_names = [f'X_{i}' for i in range(X.shape[1])] + ['y']\n",
        "df.columns = column_names\n",
        "\n",
        "print(df.head()) #prints the first 5 rows\n",
        "print(df) # prints the entire dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OKycUwxV6yT",
        "outputId": "28fda3c2-428d-4b75-a6f2-32ec682e2853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   X_0  X_1  X_2  X_3  X_4  X_5   y\n",
            "0   78   91   68   54   82   47  88\n",
            "1   60   78   97   58   62   50  60\n",
            "2   50   63   92   75   79   63  48\n",
            "3   42   61   92   41   63   83  45\n",
            "4   69   77   41   99   60   72  90\n",
            "     X_0  X_1  X_2  X_3  X_4  X_5   y\n",
            "0     78   91   68   54   82   47  88\n",
            "1     60   78   97   58   62   50  60\n",
            "2     50   63   92   75   79   63  48\n",
            "3     42   61   92   41   63   83  45\n",
            "4     69   77   41   99   60   72  90\n",
            "..   ...  ...  ...  ...  ...  ...  ..\n",
            "995   40   43   51   99   66   95  59\n",
            "996   81   62   64   60   97   91  79\n",
            "997   63   64   84   94   85   71  49\n",
            "998   94   71   44   75   85   81  58\n",
            "999   88   81   97   78   55   72  42\n",
            "\n",
            "[1000 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = model.predict(X_test[0])\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "D-sCouDRhB02",
        "outputId": "8907ffdc-f3fe-4e9a-de3c-7011b2779d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mCannot take the length of shape with unknown rank.\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  • training=False\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3fe0bd1f19ef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mCannot take the length of shape with unknown rank.\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  • training=False\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-AGZJHFjOTw",
        "outputId": "26a4c748-9b6e-4888-9d22-700ef544d044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.44067797 0.13559322 0.23728814 0.23728814 0.42372881 0.69491525]\n",
            "[78 91 68 54 82 47]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(var[0])\n",
        "print(X_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6f5cjWGjsKW",
        "outputId": "b55241bc-abbd-4d97-def7-f4a854d4f029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.77966102 0.81355932 0.69491525 0.62711864 0.28813559 0.74576271]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "num_students = 1000  # Increase the number of students for better model training\n",
        "num_exams = 10\n",
        "\n",
        "# Creating a synthetic dataset\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'student_id': range(num_students),\n",
        "    'exam_1': np.random.randint(50, 100, num_students),\n",
        "    'exam_2': np.random.randint(50, 100, num_students),\n",
        "    'exam_3': np.random.randint(50, 100, num_students),\n",
        "    'exam_4': np.random.randint(50, 100, num_students),\n",
        "    'exam_5': np.random.randint(50, 100, num_students),\n",
        "    'exam_6': np.random.randint(50, 100, num_students),\n",
        "    'exam_7': np.random.randint(50, 100, num_students),\n",
        "    'exam_8': np.random.randint(50, 100, num_students),\n",
        "    'exam_9': np.random.randint(50, 100, num_students),\n",
        "    'exam_10': np.random.randint(50, 100, num_students),\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Optional: Adding attendance and participation\n",
        "df['attendance'] = np.random.uniform(0.7, 1.0, num_students)  # Attendance as a percentage\n",
        "df['participation'] = np.random.randint(0, 2, num_students)  # 0 or 1 for participation\n",
        "\n",
        "# Save the dataset\n",
        "df.to_csv('student_marks.csv', index=False)"
      ],
      "metadata": {
        "id": "mPdtUuWx2-Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP3-3fsZ3UuJ",
        "outputId": "bcf93979-9ba9-42cf-baac-e362921e70ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     student_id  exam_1  exam_2  exam_3  exam_4  exam_5  exam_6  exam_7  \\\n",
            "0             0      88      82      52      56      68      94      80   \n",
            "1             1      78      71      55      53      74      66      52   \n",
            "2             2      64      70      58      78      71      89      57   \n",
            "3             3      92      55      55      63      80      70      73   \n",
            "4             4      57      55      58      52      66      51      64   \n",
            "..          ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "995         995      75      91      95      90      81      75      52   \n",
            "996         996      83      93      53      79      95      77      84   \n",
            "997         997      94      98      94      62      96      81      82   \n",
            "998         998      55      89      69      70      97      94      73   \n",
            "999         999      86      60      66      68      72      51      68   \n",
            "\n",
            "     exam_8  exam_9  exam_10  attendance  participation  \n",
            "0        67      94       58    0.964491              0  \n",
            "1        98      94       75    0.854558              1  \n",
            "2        73      98       51    0.932396              0  \n",
            "3        94      76       52    0.868643              0  \n",
            "4        84      88       95    0.865811              1  \n",
            "..      ...     ...      ...         ...            ...  \n",
            "995      86      61       75    0.792890              1  \n",
            "996      72      78       71    0.899120              1  \n",
            "997      78      52       80    0.899735              1  \n",
            "998      70      75       92    0.785867              0  \n",
            "999      97      79       59    0.838031              0  \n",
            "\n",
            "[1000 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted scores:\", y_pred.flatten())\n",
        "print(\"Actual scores:\", y_test[:5].flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "9pJUpjmS3tgJ",
        "outputId": "742d38e4-cca4-44dc-ff3f-91c5fc62f15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_pred' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-58751cc5d90a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example: 100 students, 6 timesteps (years), 1 feature (marks per year)\n",
        "# X = np.random.rand(100, 6, 6)\n",
        "X = np.random.randint(40, 100, size=(100, 6,6))  # Input\n",
        "y = np.random.randint(40,100,size=(100,6))     # Output (future marks)\n"
      ],
      "metadata": {
        "id": "1DkWFpYWFlkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0],y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7UhyIoAFqOU",
        "outputId": "6098941f-3335-4604-e74c-81cf1996af70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[98 92 78 50 53 62]\n",
            " [92 55 89 60 94 94]\n",
            " [43 72 93 86 53 58]\n",
            " [98 44 75 70 84 57]\n",
            " [83 70 94 69 60 86]\n",
            " [69 94 80 43 44 80]] [90 77 51 63 52 79]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scaler = MinMaxScaler()\n",
        "# X_scaled = X.reshape(-1, 1)  # Reshape to 2D for scaling\n",
        "# X_scaled = scaler.fit_transform(X_scaled).reshape(X.shape)  # Reshape back to 3D\n"
      ],
      "metadata": {
        "id": "zD-vO3VCG0ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "zcs289ZzGsmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='relu', input_shape=(6, 6)),  # 6 timesteps, 6 feature\n",
        "    Dense(6)  # six output (future marks)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXVP5jP2HBz0",
        "outputId": "53a4fb0f-fd03-4fc2-9f51-d719f3edc569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 4890.2041 - mae: 65.8690 - val_loss: 4578.0640 - val_mae: 63.9796\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 4102.8589 - mae: 59.9141 - val_loss: 3869.3198 - val_mae: 57.7947\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3294.8474 - mae: 52.8064 - val_loss: 2561.4282 - val_mae: 43.0901\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1885.5845 - mae: 37.3078 - val_loss: 1257.2177 - val_mae: 29.2729\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1091.4451 - mae: 27.0711 - val_loss: 1121.4812 - val_mae: 25.9020\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 807.0751 - mae: 22.7954 - val_loss: 824.3786 - val_mae: 22.1002\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 556.4999 - mae: 19.2899 - val_loss: 540.5805 - val_mae: 18.7730\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 478.9567 - mae: 17.7177 - val_loss: 461.4776 - val_mae: 17.2632\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 422.1005 - mae: 16.8723 - val_loss: 466.6242 - val_mae: 17.3476\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 417.6973 - mae: 16.9102 - val_loss: 455.1478 - val_mae: 17.3318\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 382.8849 - mae: 15.8628 - val_loss: 447.3130 - val_mae: 17.3962\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 360.2979 - mae: 15.3904 - val_loss: 435.8723 - val_mae: 17.3315\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 352.2388 - mae: 15.6269 - val_loss: 429.0526 - val_mae: 17.1808\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 348.6203 - mae: 15.4441 - val_loss: 432.0424 - val_mae: 17.0313\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 311.1110 - mae: 14.6051 - val_loss: 438.4086 - val_mae: 17.1174\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 309.8033 - mae: 14.5638 - val_loss: 436.6021 - val_mae: 17.0915\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 305.1893 - mae: 14.5124 - val_loss: 429.2931 - val_mae: 16.9872\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 307.9341 - mae: 14.6510 - val_loss: 424.1668 - val_mae: 16.9565\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 293.2195 - mae: 14.4665 - val_loss: 432.1911 - val_mae: 17.0764\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 295.9067 - mae: 14.3455 - val_loss: 434.8389 - val_mae: 17.2367\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 278.8813 - mae: 14.0173 - val_loss: 436.2522 - val_mae: 17.1550\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 273.7122 - mae: 13.7604 - val_loss: 430.6157 - val_mae: 17.0799\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 266.8145 - mae: 13.7294 - val_loss: 439.2870 - val_mae: 17.2599\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 272.6017 - mae: 13.9757 - val_loss: 430.0849 - val_mae: 17.1423\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 259.6196 - mae: 13.5106 - val_loss: 429.9146 - val_mae: 17.1444\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 253.3731 - mae: 13.3016 - val_loss: 447.5956 - val_mae: 17.4771\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 260.0419 - mae: 13.5340 - val_loss: 449.9547 - val_mae: 17.5763\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 255.5713 - mae: 13.4862 - val_loss: 447.5724 - val_mae: 17.5806\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 237.9817 - mae: 12.8513 - val_loss: 452.2160 - val_mae: 17.7219\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 246.0980 - mae: 13.0926 - val_loss: 445.2360 - val_mae: 17.5422\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 219.5891 - mae: 12.3580 - val_loss: 451.7328 - val_mae: 17.6807\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 245.4897 - mae: 13.0138 - val_loss: 464.7262 - val_mae: 17.9676\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 226.1203 - mae: 12.5225 - val_loss: 450.9567 - val_mae: 17.6480\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 226.8726 - mae: 12.6213 - val_loss: 461.1458 - val_mae: 17.8610\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 223.9085 - mae: 12.2794 - val_loss: 472.3095 - val_mae: 17.8959\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 216.8767 - mae: 12.4208 - val_loss: 480.9720 - val_mae: 18.1707\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 219.4402 - mae: 12.3545 - val_loss: 475.6370 - val_mae: 18.0826\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 213.7041 - mae: 12.1375 - val_loss: 484.1177 - val_mae: 18.1729\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 215.0287 - mae: 12.2464 - val_loss: 485.8854 - val_mae: 18.1550\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 191.0349 - mae: 11.2946 - val_loss: 486.7053 - val_mae: 18.1683\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 203.8079 - mae: 11.7657 - val_loss: 495.3299 - val_mae: 18.2307\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 201.0228 - mae: 11.6385 - val_loss: 503.3144 - val_mae: 18.5190\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 184.3208 - mae: 11.1642 - val_loss: 519.9703 - val_mae: 18.5975\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 190.6612 - mae: 11.4210 - val_loss: 516.6594 - val_mae: 18.5770\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 184.7603 - mae: 11.1917 - val_loss: 517.9225 - val_mae: 18.4778\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 173.5852 - mae: 10.7420 - val_loss: 522.7578 - val_mae: 18.5713\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 184.2560 - mae: 11.0774 - val_loss: 529.1395 - val_mae: 18.5710\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 164.3934 - mae: 10.5721 - val_loss: 512.7552 - val_mae: 18.3708\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 162.9940 - mae: 10.3106 - val_loss: 512.9521 - val_mae: 18.3902\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 165.3145 - mae: 10.3944 - val_loss: 538.5269 - val_mae: 18.9099\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 538.5269 - mae: 18.9099\n",
            "Test Loss: 538.5269, Test MAE: 18.9099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give the code to predict the value of the given input array\n",
        "\n",
        "# Example input array (replace with your actual input)\n",
        "input_array = np.array([[60, 70, 80, 90, 85, 75]])\n",
        "\n",
        "# Preprocess the input array using the same scaler used during training\n",
        "input_array_scaled = scaler.transform(input_array)\n",
        "\n",
        "# Make the prediction\n",
        "predicted_value = model.predict(input_array_scaled)\n",
        "\n",
        "print(\"Predicted value:\", predicted_value)"
      ],
      "metadata": {
        "id": "e7kI0BnZHWf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTtNZzORHjdl",
        "outputId": "4e652661-8151-4de7-da49-b7018e4386a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[57 46 95 59 77 50]\n",
            " [83 97 55 90 94 66]\n",
            " [90 50 55 62 99 98]\n",
            " [51 98 79 42 41 99]\n",
            " [77 87 91 63 68 86]\n",
            " [85 73 59 57 53 57]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_train[0].reshape(1,6,6) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v0ogGpSIAeO",
        "outputId": "06d655ca-dfb1-469c-a5fb-26a1f31523dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[58.42833 , 55.28693 , 71.32301 , 89.68169 , 47.858467, 94.181244]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0].reshape(1,6,6) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGm-BRzQIWQN",
        "outputId": "8739393e-c19d-4f39-b5b5-3219b5066c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[57 46 95 59 77 50]\n",
            "  [83 97 55 90 94 66]\n",
            "  [90 50 55 62 99 98]\n",
            "  [51 98 79 42 41 99]\n",
            "  [77 87 91 63 68 86]\n",
            "  [85 73 59 57 53 57]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give the code to export the model to joblib\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Assuming 'model' is your trained Keras model\n",
        "joblib.dump(model, 'my_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLSBaJdUI6P0",
        "outputId": "cec6380a-6199-4620-f38d-3d77d4d2789f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give the code to import the joblib model and to predict the output from the given array\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('my_model.joblib')\n",
        "\n",
        "# Example input array (replace with your actual input)\n",
        "input_array = np.array([[[60, 70, 80, 90, 85, 75]]])\n",
        "\n",
        "# Assuming the input shape is (1, 6, 6) for LSTM model\n",
        "# Reshape the input if necessary\n",
        "# input_array = input_array.reshape(1, 6, 6)\n",
        "\n",
        "# Make the prediction\n",
        "predicted_value = loaded_model.predict(X_train[0].reshape(1,6,6))\n",
        "\n",
        "print(\"Predicted value:\", predicted_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL6-1aWmJIBv",
        "outputId": "bf570311-05f3-4238-d02f-077eb976940a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "Predicted value: [[58.42833  55.28693  71.32301  89.68169  47.858467 94.181244]]\n"
          ]
        }
      ]
    }
  ]
}